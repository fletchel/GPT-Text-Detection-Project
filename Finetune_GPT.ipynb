{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNOtraQes5Sa"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "model_name = \"124M\"\n",
        "file_name = \"Datasets/ireland_headlines.csv\"\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "%cd 'drive/MyDrive/Internship'\n",
        "\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\n",
        "  gpt2.download_gpt2(model_name=model_name)\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess, file_name, model_name=model_name, steps=2000, print_every=10, run_name=\"ireland_headlines\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIiXfdhoIl7K"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "temps = [1, 1.25, 1.5]\n",
        "sizes = [10000, 2000, 2000]\n",
        "\n",
        "model_names = [\"ireland_headlines\"]\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "%cd 'drive/MyDrive/Internship'\n",
        "\n",
        "for m in model_names:\n",
        "\n",
        "  sess = gpt2.start_tf_sess()\n",
        "  gpt2.load_gpt2(sess, run_name=m)\n",
        "\n",
        "  for i in range(len(temps)):\n",
        "\n",
        "    t = temps[i]\n",
        "\n",
        "    sample_size = sizes[i]\n",
        "    print(sample_size)\n",
        "\n",
        "    csv_name = m + str(t).replace('.', '_')\n",
        "    samp = gpt2.generate(sess, run_name = m, nsamples=sample_size, batch_size=20, prefix=\"<|startoftext|>\", truncate=\"<|endoftext|>\", temperature = t, return_as_list=True)\n",
        "    samp = pd.DataFrame(samp)\n",
        "\n",
        "    samp.to_csv(\"Training Data/\" + csv_name + \".csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqABwcg0JCyH",
        "outputId": "acd608f5-18be-4fed-d794-5d36e108333c"
      },
      "source": [
        "#editing csv files\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "file_name = \"Datasets/News Articles/articles1.csv\"\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "%cd 'drive/MyDrive/Internship'\n",
        "\n",
        "f = pd.read_csv(file_name)\n",
        "\n",
        "f = f['content']\n",
        "\n",
        "f.to_csv(\"Datasets/articles1_edit.csv\", index=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Internship\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4XH9n2sNYuj"
      },
      "source": [
        "Need to significantly clean all this data - remove links and maybe hashtags to avoid tweets being cut short by obviously fake links."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4x_AzCdG-sJ",
        "outputId": "cb471190-1761-4fb3-8033-64ce720b06c8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "file_name = \"Datasets/tweets_edited.csv\"\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "%cd 'drive/MyDrive/Internship'\n",
        "\n",
        "f = pd.read_csv(file_name, encoding='latin-1')\n",
        "\n",
        "print(f.columns)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "new_f = f.iloc[0:50000, 5]\n",
        "print(new_f[1:10])\n",
        "\n",
        "\n",
        "new_f.to_csv(\"Datasets/tweets_edited.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Internship\n",
            "Index(['0', '1467810369', 'Mon Apr 06 22:19:45 PDT 2009', 'NO_QUERY',\n",
            "       '_TheSpecialOne_',\n",
            "       '@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D'],\n",
            "      dtype='object')\n",
            "1    @Kenichan I dived many times for the ball. Man...\n",
            "2      my whole body feels itchy and like its on fire \n",
            "3    @nationwideclass no, it's not behaving at all....\n",
            "4                        @Kwesidei not the whole crew \n",
            "5                                          Need a hug \n",
            "6    @LOLTrish hey  long time no see! Yes.. Rains a...\n",
            "7                 @Tatiana_K nope they didn't have it \n",
            "8                            @twittera que me muera ? \n",
            "9          spring break in plain city... it's snowing \n",
            "Name: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6-pfTSoVP1x",
        "outputId": "7c6a408a-6062-409e-c909-67df20549155"
      },
      "source": [
        "# Generate text and save\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "\n",
        "%cd 'drive/MyDrive/Internship'\n",
        "\n",
        "model_names = [\"tweets_edited\"]\n",
        "temps = [0.5, 0.7, 1, 1.3]\n",
        "temp_names = ['very_low', 'low', 'med', 'high']\n",
        "sample_size = 1000\n",
        "\n",
        "for m in model_names:\n",
        "\n",
        "  sess = gpt2.start_tf_sess()\n",
        "  gpt2.load_gpt2(sess, run_name=m)\n",
        "\n",
        "  for i in range(len(temps)):\n",
        "\n",
        "    t = temps[i]\n",
        "\n",
        "    sample\n",
        "\n",
        "    csv_name = m + str(t).replace('.', '_') + '.csv'\n",
        "\n",
        "    samp = gpt2.generate(sess, run_name = m, nsamples=sample_size, batch_size=20, prefix=\"<|startoftext|>\", truncate=\"<|endoftext|>\", temperature = t, return_as_list=True)\n",
        "    print(1)\n",
        "    samp = pd.DataFrame(samp)\n",
        "\n",
        "    samp.to_csv(\"Training Data/\" + csv_name + \".csv\", index=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Internship\n",
            "Loading checkpoint checkpoint/tweets_edited/model-1000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/tweets_edited/model-1000\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuWAh3Rd8VQv",
        "outputId": "731a07b6-1f74-40a0-feb8-f1a7554814dd"
      },
      "source": [
        "# getting sizes of datasets\n",
        "\n",
        "!pip install -q gpt-2-simple\n",
        "\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "\n",
        "%cd 'drive/MyDrive/Internship/Datasets'\n",
        "\n",
        "file_names = [\"articles1p_edit.csv\", \"fb_edit.csv\", \"ireland_headlines.csv\", \"tweets_edited.csv\"]\n",
        "\n",
        "for f in file_names:\n",
        "\n",
        "  a = pd.read_csv(f)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Internship/Datasets\n",
            "40855\n",
            "99999\n",
            "521836\n",
            "1599999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}