# GPT-Text-Detection-Project

Project done during the 2021 Hamilton Internship in TCD.

The aim of the project was to see in which contexts and under what conditions GPT-2 generated text could be distinguished from human text. A BERT classifier was compared with a baseline of Logistic Regression on word embeddings. A poster summarising findings can be seen below.

(Note: this repo only contains the training data generator and the code for the LR baseline. The BERT classifier was trained in a colab notebook and I'll probably get around to putting it up at some point too. All corpora/pre-trained embeddings were deleted before uploading here as they were very large but links can be found 
in the report for anyone interested. Also be warned that the code here is not super clean or well commented!!)

![InternshipPoster](https://user-images.githubusercontent.com/70916204/137162932-0bfbac32-e606-4838-874c-d9c2ce857cfb.jpg)
